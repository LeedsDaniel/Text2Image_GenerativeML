{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "from discriminator_utils import COCO2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGenerator(nn.Module):\n",
    "    # define constructor for the generator\n",
    "    def __init__(self, noise_in_dimension, noise_out_dimension, vocab_size, embedding_length=128):\n",
    "        # inherit nn.Module class methods\n",
    "        super().__init__()\n",
    "        # set class attributes:\n",
    "        # model branch at start of model structure for noise (image)\n",
    "        self.initial_noise_processing = nn.Sequential(nn.Linear(in_features=noise_in_dimension, out_features=4*4*noise_out_dimension, bias=True), nn.CELU(alpha=1, inplace=True))\n",
    "        # moddel branch at start of model structure (parallel to noise branch) for condition (text)\n",
    "        self.initial_condition_processing = nn.Sequential(nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_length, padding_idx=1), nn.Linear(in_features=embedding_length, out_features=16), nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        self.generator_model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=59+noise_out_dimension, out_channels=64*8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64*8), # momentum parameter\n",
    "            nn.ReLU(inplace=True), # 64x512x8x8\n",
    "            nn.ConvTranspose2d(in_channels=64*8, out_channels=64*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64*4),\n",
    "            nn.ReLU(inplace=True), # 64x256x16x16\n",
    "            nn.ConvTranspose2d(in_channels=64*4, out_channels=64*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64*2),\n",
    "            nn.ReLU(inplace=True), # 64x128x32x32\n",
    "            # nn.ConvTranspose2d(in_channels=64*2, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            # nn.BatchNorm2d(num_features=64),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh() #64x3x64x64\n",
    "            )\n",
    "        \n",
    "    def forward(self, noise, text):\n",
    "        preconcat_noise = self.initial_noise_processing(noise)\n",
    "        # reshape noise data\n",
    "        preconcat_noise = torch.reshape(preconcat_noise, (-1, 512, 4, 4))\n",
    "        preconcat_text = self.initial_condition_processing(text)\n",
    "        # reshape text data\n",
    "        preconcat_text = torch.reshape(preconcat_text, (64, 944))\n",
    "        preconcat_text = torch.reshape(preconcat_text, (-1, 59, 4, 4))\n",
    "        # concatenate the image data and text data\n",
    "        image_and_text = torch.concat((preconcat_noise, preconcat_text), dim=1)\n",
    "        generative_result = self.generator_model(image_and_text)\n",
    "        return generative_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf, opt):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.nc = nc\n",
    "        self.ndf = ndf\n",
    "        self.convD = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.fcD = nn.Sequential(\n",
    "            nn.Linear(opt['txtSize'], opt['nt']),\n",
    "            nn.BatchNorm1d(opt['nt']),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.netD = nn.Sequential(\n",
    "            nn.Conv2d(ndf * 8 + opt['nt'], ndf * 8, kernel_size=1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, kernel_size=4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, txt):\n",
    "        conv_output = self.convD(img)\n",
    "        replicated_txt = self.fcD(txt.float())\n",
    "        concatenated_input = torch.cat((conv_output, replicated_txt.unsqueeze(2).unsqueeze(3).repeat(1, 1, conv_output.size(2), conv_output.size(3))), dim=1)\n",
    "        output = self.netD(concatenated_input)\n",
    "        return output.view(-1, 1).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (convD): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (fcD): Sequential(\n",
      "    (0): Linear(in_features=59, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (netD): Sequential(\n",
      "    (0): Conv2d(640, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      ")\n",
      "CGenerator(\n",
      "  (initial_noise_processing): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=8192, bias=True)\n",
      "    (1): CELU(alpha=1, inplace=True)\n",
      "  )\n",
      "  (initial_condition_processing): Sequential(\n",
      "    (0): Embedding(24784, 128, padding_idx=1)\n",
      "    (1): Linear(in_features=128, out_features=16, bias=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (generator_model): Sequential(\n",
      "    (0): ConvTranspose2d(571, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dataset = COCO2017('./train_annotations.json', r'C:\\Users\\danie\\Documents\\Data\\COMP652\\FinalProject\\train2017', 64000)\n",
    "data_loader = DataLoader(dataset, batch_size=64) # shuffle=True\n",
    "\n",
    "# dictionary for discriminator model parameters\n",
    "opt = {\n",
    "    'txtSize': 59, # Determined by max_length in tokenizing\n",
    "    'nt': 128,\n",
    "    'ndf': 64,\n",
    "    'nc': 3, # number of input channels, for RGB images in COCO is 3x64x64\n",
    "}\n",
    "# assign an instance of the Discriminator Class to a variable\n",
    "discriminator = Discriminator(opt['nc'], opt['ndf'], opt).to('cuda')\n",
    "print(discriminator)\n",
    "# assign an instance of the CGenerator Class to a variable\n",
    "generator = CGenerator(noise_in_dimension=100, noise_out_dimension=512, vocab_size=24784).to('cuda')\n",
    "print(generator)\n",
    "# define the loss function\n",
    "loss = nn.BCELoss()\n",
    "# define optimizers with learning rate and momentum (Beta1) defined in research paper for the generator\n",
    "discriminator_optimizer = optim.Adam(params=discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "generator_optimizer = optim.Adam(params=generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0==> Duration (sec): 14.2. Average Batch Loss (Discriminator): 0.8571754950318635. Average Batch Loss (Generator): 3.3201395372748377.\n",
      "Epoch 1==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.8166059298200375. Average Batch Loss (Generator): 3.1149997739046813.\n",
      "Epoch 2==> Duration (sec): 12.9. Average Batch Loss (Discriminator): 0.7617775377637175. Average Batch Loss (Generator): 3.1803221253504357.\n",
      "Epoch 3==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.7207302550051988. Average Batch Loss (Generator): 3.2325179138686506.\n",
      "Epoch 4==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.6867798178899086. Average Batch Loss (Generator): 3.298916923199594.\n",
      "Epoch 5==> Duration (sec): 13.4. Average Batch Loss (Discriminator): 0.6569176209905914. Average Batch Loss (Generator): 3.369518345706165.\n",
      "Epoch 6==> Duration (sec): 12.9. Average Batch Loss (Discriminator): 0.6307448239297907. Average Batch Loss (Generator): 3.46123771426401.\n",
      "Epoch 7==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.6089904968027443. Average Batch Loss (Generator): 3.5350160396420396.\n",
      "Epoch 8==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.5925356903568572. Average Batch Loss (Generator): 3.593792867558284.\n",
      "Epoch 9==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.5813540017899567. Average Batch Loss (Generator): 3.6306421428600326.\n",
      "Epoch 10==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.5704820597250937. Average Batch Loss (Generator): 3.6552484122482554.\n",
      "Epoch 11==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.5622471293682972. Average Batch Loss (Generator): 3.6765656354447516.\n",
      "Epoch 12==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.5516474055448621. Average Batch Loss (Generator): 3.6976480344188616.\n",
      "Epoch 13==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.5428812061188609. Average Batch Loss (Generator): 3.724388475932048.\n",
      "Epoch 14==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.53494036203635. Average Batch Loss (Generator): 3.7471492943841964.\n",
      "Epoch 15==> Duration (sec): 13.3. Average Batch Loss (Discriminator): 0.5264647867226431. Average Batch Loss (Generator): 3.7694489571329903.\n",
      "Epoch 16==> Duration (sec): 13.4. Average Batch Loss (Discriminator): 0.5208172577684582. Average Batch Loss (Generator): 3.7961360947199383.\n",
      "Epoch 17==> Duration (sec): 13.5. Average Batch Loss (Discriminator): 0.5111550835454206. Average Batch Loss (Generator): 3.8190977132013066.\n",
      "Epoch 18==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.5038669857188424. Average Batch Loss (Generator): 3.844449607692365.\n",
      "Epoch 19==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.4952525225817293. Average Batch Loss (Generator): 3.8718896100484765.\n",
      "Epoch 20==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.4859624933106473. Average Batch Loss (Generator): 3.9027325353268534.\n",
      "Epoch 21==> Duration (sec): 13.2. Average Batch Loss (Discriminator): 0.4766490013963774. Average Batch Loss (Generator): 3.9348740710657597.\n",
      "Epoch 22==> Duration (sec): 13.2. Average Batch Loss (Discriminator): 0.4684306775844562. Average Batch Loss (Generator): 3.968696788644742.\n",
      "Epoch 23==> Duration (sec): 13.2. Average Batch Loss (Discriminator): 0.4593024382410496. Average Batch Loss (Generator): 4.004324729741939.\n",
      "Epoch 24==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.4505488260623518. Average Batch Loss (Generator): 4.0424882898242025.\n",
      "Epoch 25==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.44256444863852523. Average Batch Loss (Generator): 4.0852914697156075.\n",
      "Epoch 26==> Duration (sec): 13.2. Average Batch Loss (Discriminator): 0.43484830054319334. Average Batch Loss (Generator): 4.1220231480560505.\n",
      "Epoch 27==> Duration (sec): 13.3. Average Batch Loss (Discriminator): 0.4265168562378058. Average Batch Loss (Generator): 4.164370713972154.\n",
      "Epoch 28==> Duration (sec): 13.2. Average Batch Loss (Discriminator): 0.4187394441219577. Average Batch Loss (Generator): 4.205563532258481.\n",
      "Epoch 29==> Duration (sec): 13.5. Average Batch Loss (Discriminator): 0.4105137182843135. Average Batch Loss (Generator): 4.251343827295862.\n",
      "Epoch 30==> Duration (sec): 13.4. Average Batch Loss (Discriminator): 0.40471844735133194. Average Batch Loss (Generator): 4.292489273681335.\n",
      "Epoch 31==> Duration (sec): 13.3. Average Batch Loss (Discriminator): 0.39717449640234226. Average Batch Loss (Generator): 4.327832309255202.\n",
      "Epoch 32==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.38905544018421145. Average Batch Loss (Generator): 4.372115721707825.\n",
      "Epoch 33==> Duration (sec): 13.2. Average Batch Loss (Discriminator): 0.38288115225380076. Average Batch Loss (Generator): 4.4162922628327435.\n",
      "Epoch 34==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.3768275763169248. Average Batch Loss (Generator): 4.457155011871723.\n",
      "Epoch 35==> Duration (sec): 13.4. Average Batch Loss (Discriminator): 0.37039853714307824. Average Batch Loss (Generator): 4.495578128153406.\n",
      "Epoch 36==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.3644087241689134. Average Batch Loss (Generator): 4.5419924321486995.\n",
      "Epoch 37==> Duration (sec): 13.0. Average Batch Loss (Discriminator): 0.3588894454307521. Average Batch Loss (Generator): 4.581477021572842.\n",
      "Epoch 38==> Duration (sec): 12.9. Average Batch Loss (Discriminator): 0.3528041914610806. Average Batch Loss (Generator): 4.628163476249943.\n",
      "Epoch 39==> Duration (sec): 12.9. Average Batch Loss (Discriminator): 0.3471515299302877. Average Batch Loss (Generator): 4.670251421990082.\n",
      "Epoch 40==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.3406885156790549. Average Batch Loss (Generator): 4.722524822760487.\n",
      "Epoch 41==> Duration (sec): 13.3. Average Batch Loss (Discriminator): 0.33634567269081356. Average Batch Loss (Generator): 4.767971809323944.\n",
      "Epoch 42==> Duration (sec): 13.3. Average Batch Loss (Discriminator): 0.33124825034241867. Average Batch Loss (Generator): 4.816861451905026.\n",
      "Epoch 43==> Duration (sec): 13.2. Average Batch Loss (Discriminator): 0.32690476595603085. Average Batch Loss (Generator): 4.856628160733285.\n",
      "Epoch 44==> Duration (sec): 13.5. Average Batch Loss (Discriminator): 0.3220652810960426. Average Batch Loss (Generator): 4.902461738135976.\n",
      "Epoch 45==> Duration (sec): 13.1. Average Batch Loss (Discriminator): 0.3184454821546833. Average Batch Loss (Generator): 4.9368790517079075.\n",
      "Epoch 46==> Duration (sec): 13.2. Average Batch Loss (Discriminator): 0.31444117343166056. Average Batch Loss (Generator): 4.98129453536985.\n",
      "Epoch 47==> Duration (sec): 13.5. Average Batch Loss (Discriminator): 0.31101649328502706. Average Batch Loss (Generator): 5.022220686652911.\n",
      "Epoch 48==> Duration (sec): 13.7. Average Batch Loss (Discriminator): 0.3071018692098302. Average Batch Loss (Generator): 5.065753673903701.\n",
      "Epoch 49==> Duration (sec): 13.4. Average Batch Loss (Discriminator): 0.30331679240831183. Average Batch Loss (Generator): 5.100737444624808.\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 50\n",
    "batch_size = 64\n",
    "# discriminator_batch_loss_real_list = []\n",
    "# discriminator_batch_loss_synthetic_list = []\n",
    "discriminator_batch_loss_list = []\n",
    "generator_batch_loss_list = []\n",
    "discriminator_epoch_loss_list = []\n",
    "generator_epoch_loss_list = []\n",
    "best_generator_loss = np.inf\n",
    "best_discriminator_loss = np.inf\n",
    "for epoch in range(epoch_count): # train GAN epoch_count number of times\n",
    "    epoch_start = time.time()\n",
    "    for idx, (image, text) in enumerate(data_loader, 0): # iterate through the training data loader (get batched data)\n",
    "        # batch_start = time.time()\n",
    "        image = image.to('cuda')\n",
    "        text = text.to('cuda')\n",
    "        text_4g = torch.unsqueeze(text, 1).long()\n",
    "        # set gradient for all discriminator parametrs to None\n",
    "        discriminator.zero_grad()\n",
    "        # get discriminator predictions\n",
    "        predictions_real = discriminator(image, text)\n",
    "        # vector of ones\n",
    "        labels_real = torch.ones(predictions_real.size(0)).to('cuda')\n",
    "        # get discriminator loss on real images\n",
    "        d_batch_loss_real = loss(predictions_real, labels_real)\n",
    "        # backwards propogation for discriminator (these gardients will be accumulated with the gradients from the synthetic image results)\n",
    "        d_batch_loss_real.backward()\n",
    "        # get noise from normal distribution for this batch round\n",
    "        noise = torch.randn(batch_size, 100, device='cuda')\n",
    "        # generate synthetic images\n",
    "        synthetic = generator(noise, text_4g)\n",
    "        # get discriminator predictions on synthetic images\n",
    "        predictions_synthetic = discriminator(synthetic.detach(), text)\n",
    "        # vector of zeros\n",
    "        labels_synthetic = torch.zeros(predictions_synthetic.size(0)).to('cuda')\n",
    "        # get discriminator loss on synthetic images\n",
    "        d_batch_loss_synthetic = loss(predictions_synthetic, labels_synthetic)\n",
    "        # backwards propogation for discriminator (these gardients are accumulated with the gradients from the real image results)\n",
    "        d_batch_loss_synthetic.backward()\n",
    "        # update discriminator\n",
    "        discriminator_optimizer.step()  \n",
    "        # set gradient for all generator parametrs to None\n",
    "        generator.zero_grad(set_to_none=True)\n",
    "        # get discriminator predictions on synthetic images\n",
    "        predictions_synthetic_updated = discriminator(synthetic, text)\n",
    "        # generator loss\n",
    "        generator_batch_loss = loss(predictions_synthetic_updated, labels_real)\n",
    "        # backwards propogation for generator\n",
    "        generator_batch_loss.backward()\n",
    "        # update generator\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # batch_end = time.time()\n",
    "        # append discriminator batch loss\n",
    "        discriminator_batch_loss_list.append(d_batch_loss_synthetic.item() + d_batch_loss_real.item()) \n",
    "        # append generator batch loss\n",
    "        generator_batch_loss_list.append(generator_batch_loss.item())\n",
    "\n",
    "        # print update every 300 batches\n",
    "        # if idx % 300 == 0:\n",
    "        #     print(f\"Batch # {idx}==> Batch Loss (Discriminator): {discriminator_batch_loss_list[idx]}. Batch Loss (Generator): {generator_batch_loss}.\") # Cumulative Average Batch Loss (Generator): {np.mean(generator_batch_loss_list)}\n",
    "    epoch_end = time.time()\n",
    "    print(f\"Epoch {epoch}==> Duration (sec): {round((epoch_end - epoch_start),1)}. Average Batch Loss (Discriminator): {np.mean(discriminator_batch_loss_list)}. Average Batch Loss (Generator): {np.mean(generator_batch_loss_list)}.\")\n",
    "    generator_epoch_loss_list.append(np.mean(generator_batch_loss_list))\n",
    "    discriminator_epoch_loss_list.append(np.mean(discriminator_batch_loss_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del generator\n",
    "del discriminator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
